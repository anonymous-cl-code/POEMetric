import pandas as pd
import os
import openai
from tqdm import tqdm
import json
from multiprocessing import Pool
import argparse

# Set up argument parser
parser = argparse.ArgumentParser(description='Poetry evaluation script.')
parser.add_argument('--api_key', required=True, help='API Key for OpenAI')
parser.add_argument('--model', required=True, help='Model name to use')
parser.add_argument('--excel_file', required=True, help='Path to the Excel file')
parser.add_argument('--azure_endpoint', required=True, help='Azure endpoint')
parser.add_argument('--base_dir', required=True, help='Base directory for output')

args = parser.parse_args()

client = openai.AzureOpenAI(
    azure_endpoint=args.azure_endpoint,
    api_version="2023-07-01-preview",
    api_key=args.api_key,
)

def create_chat_completion(model=args.model, text=""):
    messages = [{
        "role": "user",
        "content": f"{text}"
    }]
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=8000
    )
    return completion

df = pd.read_excel(args.excel_file)

column_titles = df.columns.tolist()
participants = [title.split("_poem")[0] for title in column_titles if 'poem' in title]
print(participants)

# Prompt template
template = '''
# Role Description
You are a professional poetry critic and analyst. Your job is to evaluate English poetry written by human beings and English poetry generated by large language models. 

# Task Definition
I will ask you to evaluate one poem by answering 10 multiple-choice questions and 3 open-ended questions, without telling you if the poem is written by a human or an LLM. For each multiple-choice question, please score the poem from 1 to 5 based on how strongly you agree or disagree with the question.  
# Task Procedures
- Step 1: Read the prompt (i.e., the instructions that were given for writing each of the specific poems).
- Step 2: Read the poem that was written in response to this prompt. 
- Step 3: Answer the questions that follow. 

# Context

## The prompt
{the_prompt}

## The poem
{the_poem}

## Questions
1. The poem follows the given prompt in terms of form, including meter and rhyme where applicable.
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

2. The poem follows the given prompt in terms of its theme.
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

3. The poem uses a varied vocabulary. 
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

4. The poem is a creative work. 
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

5. This poem shows idiosyncrasy.
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

6. This poem evokes emotional resonance.
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

7. The imagery in this poem is used well.
0 - N/A (No imagery is used)
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

8. At least one of the literary devices listed below was used well in the poem. 
- Simile
- Metaphor
- Personification 
- Allusion
0 - N/A (No literary devices are used)
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

9. Please comment on why you gave the answer that you did for question 8 above. 

10. This is a good poem.
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

11. Please comment on why you gave the answer that you did for question 10 above.
12. The poem is written by a human. 
1 - Strongly disagree
2 - Disagree
3 - Neutral
4 - Agree
5 - Strongly agree

13. Please give comments on why you gave the answer that you did for question 12 above. 

## Output Format
For each multiple-choice question, please give your score directly, without any explanation. Your output should be in the json format as follows: 
'''
post_prompt = "{\"1\": <insert your score here>, \"2\": <insert your score here>, ..., \"9\": \"<insert your comments here>\", ...}"

# Create directories for prompts and evaluation results
prompt_output_dir = os.path.join(args.base_dir, f'poem_prompts_{args.model}')
result_output_dir = os.path.join(args.base_dir, f'poem_evaluations_{args.model}')
os.makedirs(prompt_output_dir, exist_ok=True)
os.makedirs(result_output_dir, exist_ok=True)

def is_valid(raw_str):
    cleaned_str = raw_str.strip().strip('`').strip('json').strip().strip("\n")
    if cleaned_str.endswith('}'):
        return True

# Define function to process a single task
def process_task(args):
    row, participant, prompt_output_dir, result_output_dir, template, post_prompt = args

    participant_index = row[f"{participant}_index"]
    participant_poem = row[f"{participant}_poem"]
    prompt = row['prompt']

    prompt_file_name = os.path.join(prompt_output_dir, f"prompt_{participant_index}.txt")
    result_file_name = os.path.join(result_output_dir, f"evaluation_{participant_index}.txt")

    if os.path.exists(result_file_name):
        with open(result_file_name, 'r', encoding='utf-8') as f:
            result = f.read()
        if is_valid(result):
            return f"skip: {result_file_name}"

    try:
        evaluation_prompt = template.format(the_prompt=prompt, the_poem=participant_poem) + post_prompt
        with open(prompt_file_name, 'w', encoding='utf-8') as f:
            f.write(evaluation_prompt)

        response = create_chat_completion(text=evaluation_prompt).model_dump()
        evaluation_result = response['choices'][0]['message']['content']

        if not is_valid(evaluation_result):
            return f"invalid: {evaluation_result}"

        with open(result_file_name, 'w', encoding='utf-8') as f:
            f.write(evaluation_result)

        return f"success: {result_file_name}"
    except Exception as e:
        return f"error: {str(e)}"

if __name__ == "__main__":
    total_tasks = len(df) * len(participants)
    main_progress = tqdm(total=total_tasks, desc="Overall Progress", unit="task")

    all_tasks = []
    for index, row in df.iterrows():
        tasks = [
            (row, participant, prompt_output_dir, result_output_dir, template, post_prompt)
            for participant in participants
        ]
        all_tasks.extend(tasks)

    ret = process_task(all_tasks[0])

    with Pool(processes=8) as pool:
        for result in tqdm(pool.imap_unordered(process_task, all_tasks), total=total_tasks):
            tqdm.write(result)
            main_progress.update(1)
    main_progress.close()
    print("All evaluation tasks are completed.")